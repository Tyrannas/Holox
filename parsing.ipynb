{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Lexems file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs = require('fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer_str = fs.readFileSync('symbols.ho').toString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the lexems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tokenizer which each regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = {}\n",
    "lexems = lexer_str.split(';')\n",
    "if(lexems[lexems.length - 1] === '') lexems.pop()\n",
    "lexems.forEach(l => { \n",
    "    let parsed = l.split(':=')\n",
    "    console.log(parsed)\n",
    "    regex = \"^\" + parsed[1].trim()\n",
    "    tokenizer[parsed[0].trim()] = new RegExp(regex)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function toTokens(phrase){\n",
    "    let tokens = []\n",
    "    for(cursor = 0; cursor < phrase.length; ++cursor){\n",
    "        // ignore spaces\n",
    "        if(phrase[cursor] === \" \"){\n",
    "            continue\n",
    "        }\n",
    "        let found = false\n",
    "        for (let el of Object.entries(tokenizer)){\n",
    "            let regex = el[1]\n",
    "            let tokenName = el[0]\n",
    "            let res = phrase.slice(cursor).match(regex)\n",
    "            if(res){\n",
    "                console.log(\"wow we found a \" + tokenName + \" with value \" + res)\n",
    "                tokens.push({type: tokenName, value: res[0]})\n",
    "                found = true\n",
    "                cursor += res[0].length - 1\n",
    "                break\n",
    "            }\n",
    "        }\n",
    "        if(!found){\n",
    "            console.log(\"arf, could not tokenize character \" + phrase[cursor])\n",
    "        }\n",
    "    }\n",
    "    return tokens\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"(Faisan + Multicolore):1 \\\\> Etre -> Oiseau, *Chasseurs* -> (Chasser + Beaucoup) -> @1.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = toTokens(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_str = fs.readFileSync('grammar.ho').toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {}\n",
    "rules = grammar_str.split(';')\n",
    "rules.pop()\n",
    "rules.forEach(r => { \n",
    "    let parsed = r.split(':=')\n",
    "    let tokens = parsed[1].split('|').map(tok => tok.trim().split(' '))\n",
    "    grammar[parsed[0].trim()] = tokens\n",
    "})\n",
    "grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on prend un token\n",
    "on prend toutes les regles qui commencent par ce token\n",
    "pour chaque regle, tant qu'elles peuvent consommer des token on continue Ã  prendre des tokens\n",
    "( mot + mot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function parse(tokens, rules){\n",
    "    console.log(\"RULES\")\n",
    "    console.log(tokens, rules)\n",
    "    if(tokens.length === 0){\n",
    "        console.log(\"no more tokens\")\n",
    "        return\n",
    "    }\n",
    "    for(rule in rules){\n",
    "        filter_rules = rules[rule].filter(r => r[0] == tokens[0].type)\n",
    "        if(filter_rules.length > 0){\n",
    "            console.log(tokens[0], filter_rules)\n",
    "            newRules = {}\n",
    "            newRules[rule] = filter_rules\n",
    "            parse(tokens.slice(1, ), newRules)\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'texte := ( phrase fin_phrase ) + ;\\n phrase := mot id ?;'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_grammar = \"texte := ( phrase fin_phrase ) + ;\\n phrase := mot id ?;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Javascript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "14.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
